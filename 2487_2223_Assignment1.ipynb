{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2487-2223 Machine Learning Assignment 1\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (25 points) - Zestimate this House\n",
    "\n",
    "Purchasing a house is a very big decision for most of us. Companies such as Zillows collected tons of data regarding the listing and sold price of American houses and build the predictive model, named *Zestimate*. You are expected to build a model similar as Zestimate to predict house price in Boston. \n",
    "\n",
    "![zestimate](https://i0.wp.com/www.housesoldeasy.com/wp-content/uploads/Screen-Shot-2016-08-15-at-7.22.09-PM.png?resize=300%2C258&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded correctly.\n",
      "Features: X. Target variable (price): y.\n",
      "X shape:  (506, 13) y shape:  (506,)\n"
     ]
    }
   ],
   "source": [
    "### DON'T MODIFY - LOAD DATA ### \n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\" \n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]]) # FEATURES \n",
    "y = raw_df.values[1::2, 2] # TARGET VARIABLE\n",
    "assert X.shape[0] == y.shape[0], 'Mismatch in number of examples.'\n",
    "print('Data loaded correctly.')\n",
    "print('Features: X. Target variable (price): y.')\n",
    "print('X shape: ',X.shape, 'y shape: ', y.shape)\n",
    "### END ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns:\n",
    "| Index | Variable | Description                                                 |\n",
    "|-------|----------|-------------------------------------------------------------|\n",
    "| 0     | CRIM     | per capita crime rate by town                              |\n",
    "| 1     | ZN       | proportion of residential land zoned for lots over 25,000 sq.ft. |\n",
    "| 2     | INDUS    | proportion of non-retail business acres per town             |\n",
    "| 3     | CHAS     | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) |\n",
    "| 4     | NOX      | nitric oxides concentration (parts per 10 million)           |\n",
    "| 5     | RM       | average number of rooms per dwelling                         |\n",
    "| 6     | AGE      | proportion of owner-occupied units built prior to 1940       |\n",
    "| 7     | DIS      | weighted distances to five Boston employment centres        |\n",
    "| 8     | RAD      | index of accessibility to radial highways                    |\n",
    "| 9     | TAX      | full-value property-tax rate per $10,000                     |\n",
    "| 10    | PTRATIO  | pupil-teacher ratio by town                                  |\n",
    "| 11    | B        | 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town |\n",
    "| 12    | LSTAT    | % lower status of the population                             |\n",
    "| 13    | MEDV     | Median value of owner-occupied homes in $1000's              |\n",
    "\n",
    "Label = y = MEDV (price)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.1 (5 points) \n",
    "Create train and test set, each contains 80% and 20% of the dataset, respectively, using *train_test_split* function in scikit-learn. Train a linear model on the train set and evaluate on the test set, report the training error and test error, respectively (as mean squared error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 21.555648194527308\n",
      "Test error: 24.318238309170436\n"
     ]
    }
   ],
   "source": [
    "seed = 13\n",
    "\n",
    "# create linear regression object\n",
    "reg = LinearRegression()\n",
    "\n",
    "# split data into training and testing data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = seed)\n",
    "\n",
    "# train the model with training set to find the best parameters for the best-fitting line\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# make predictions with train set and compute MSE on train data\n",
    "y_pred_test = reg.predict(X_train)\n",
    "error_train = mean_squared_error(y_train, y_pred_test)\n",
    "    \n",
    "# make predictions with test data and compute MSE on test data (unseen)\n",
    "y_pred = reg.predict(X_test)\n",
    "error_test = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print('Training error:', error_train) # how well the model fits the training data\n",
    "print('Test error:', error_test) # how well the model can predict new, unseen data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.2 (5 points)\n",
    "\n",
    "Perform a 10-fold cross-validation on the whole data set. Show the averaged mean sqaured error on both train and test set at each fold. Explain your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_MSE</th>\n",
       "      <th>test_MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.363228</td>\n",
       "      <td>9.286947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22.882034</td>\n",
       "      <td>14.151283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.216075</td>\n",
       "      <td>14.073606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.771703</td>\n",
       "      <td>35.206924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.335426</td>\n",
       "      <td>31.885117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22.363700</td>\n",
       "      <td>19.835878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23.327221</td>\n",
       "      <td>9.947269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11.959197</td>\n",
       "      <td>168.375380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21.586295</td>\n",
       "      <td>33.329745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>23.189043</td>\n",
       "      <td>10.960411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_MSE    test_MSE\n",
       "0  23.363228    9.286947\n",
       "1  22.882034   14.151283\n",
       "2  23.216075   14.073606\n",
       "3  20.771703   35.206924\n",
       "4  21.335426   31.885117\n",
       "5  22.363700   19.835878\n",
       "6  23.327221    9.947269\n",
       "7  11.959197  168.375380\n",
       "8  21.586295   33.329745\n",
       "9  23.189043   10.960411"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform 10-fold cross-validation on dataset with MSE\n",
    "scores = cross_validate(reg, X, y, scoring='neg_mean_squared_error', cv=10, return_train_score = True)\n",
    "\n",
    "# get train and test MSE from scores dict, multiply by -1 to get positive MSE\n",
    "train_MSE = scores[\"train_score\"] * -1  # The score array for train scores on each cv split\n",
    "test_MSE = scores[\"test_score\"] * -1    # The score array for test scores on each cv split\n",
    "\n",
    "# create dataframe containing the average MSE for train and test set for each fold\n",
    "mse_df = pd.DataFrame({'train_MSE': train_MSE, 'test_MSE': test_MSE})\n",
    "mse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training error over 10-fold cross-validation: 21.39939241710581\n",
      "Average testing error over 10-fold cross-validation: 34.70525594452485\n"
     ]
    }
   ],
   "source": [
    "print(\"Average training error over 10-fold cross-validation:\", train_MSE.mean())\n",
    "print(\"Average testing error over 10-fold cross-validation:\", test_MSE.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings: the model's performance is not consistent across folds. The test error is in most folds smaller than the training error, which is an indicator that in those folds the model does generalize well to new data. \n",
    "<span style=\"color:red; font-size:20px\">Do it later</span>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.3 (5 points) \n",
    " \n",
    "Add 2-degree squared polynomial features (with no interactions) and perform 10-fold cross-validation on the whole data set. Show the mean sqaured error on both train and test set at each fold. Explain your findings.\n",
    "\n",
    "Hint: you may use sklearn.preprocessing.PolynomialFeatures and check how it produces the polynomial features with/without interaction terms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Trying out behavior of interaction terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 92)\n",
      "Only interaction features (the product of different input features) and the terms with power 1 are produced\n"
     ]
    }
   ],
   "source": [
    "poly_interaction = PolynomialFeatures(degree = 2, interaction_only = True)\n",
    "X_interaction = poly_interaction.fit_transform(X)\n",
    "print(X_interaction.shape)\n",
    "print(\"Only interaction features (the product of different input features) and the terms with power 1 are produced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 105)\n",
      "Includes 78 interaction features, 13 features with power of 1, 13 features with power of 2, and intercept term\n"
     ]
    }
   ],
   "source": [
    "poly_no_interaction = PolynomialFeatures(degree = 2, interaction_only = False)\n",
    "X_no_interaction = poly_no_interaction.fit_transform(X)\n",
    "print(X_no_interaction.shape)\n",
    "print(\"Includes 78 interaction features, 13 features with power of 1, 13 features with power of 2, and intercept term\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 26)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X = np.hstack((X, X**2))\n",
    "new_X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: to achieve no interaction, one must transform X by using np.hstack. I understood it in a way that you want no interaction features at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_MSE</th>\n",
       "      <th>test_MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.980271</td>\n",
       "      <td>10.091861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.969661</td>\n",
       "      <td>8.791095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.204169</td>\n",
       "      <td>11.458221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.665059</td>\n",
       "      <td>22.518524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.438738</td>\n",
       "      <td>14.223065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.940824</td>\n",
       "      <td>8.780146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.859624</td>\n",
       "      <td>12.953433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.419826</td>\n",
       "      <td>104.037379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.605032</td>\n",
       "      <td>13.610872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14.050919</td>\n",
       "      <td>50.771795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_MSE    test_MSE\n",
       "0  14.980271   10.091861\n",
       "1  14.969661    8.791095\n",
       "2  15.204169   11.458221\n",
       "3  13.665059   22.518524\n",
       "4  14.438738   14.223065\n",
       "5  14.940824    8.780146\n",
       "6  14.859624   12.953433\n",
       "7   7.419826  104.037379\n",
       "8  14.605032   13.610872\n",
       "9  14.050919   50.771795"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save an instance of PolynomialFeatures with the given degree\n",
    "polynomial_features = PolynomialFeatures(degree = 2, interaction_only = False)\n",
    "\n",
    "# create polynomial regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# fit model to data, using the \n",
    "model.fit(new_X, y)\n",
    "\n",
    "# perform 10-fold cross-validation on dataset with MSE\n",
    "scores_poly = cross_validate(model, new_X, y, scoring='neg_mean_squared_error', cv=10, return_train_score = True)\n",
    "\n",
    "# get train and test MSE from scores dict, multiply by -1 to get positive MSE\n",
    "train_MSE_poly = scores_poly[\"train_score\"] * -1 \n",
    "test_MSE_poly = scores_poly[\"test_score\"] * -1 \n",
    "\n",
    "# create dataframe containing the average MSE for train and test set for each fold\n",
    "mse_df_poly = pd.DataFrame({'train_MSE': train_MSE_poly, 'test_MSE': test_MSE_poly})\n",
    "mse_df_poly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training error over 10-fold cross-validation: 13.913412317032257\n",
      "Average testing error over 10-fold cross-validation: 25.72363917701756\n"
     ]
    }
   ],
   "source": [
    "print(\"Average training error over 10-fold cross-validation:\", train_MSE_poly.mean())\n",
    "print(\"Average testing error over 10-fold cross-validation:\", test_MSE_poly.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-size:20px\">Explain your findings.</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1.4 (10 points)\n",
    "\n",
    "Perform cross-validation using ridge regression and lasso regression on different feature combinations (linear features vs. polynomial features obtained earlier respectively. Explain which method works better in this case. Check the coefficients and explain the differences between ridge regression and lasso regression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression - Linear Features:\n",
      "Average training error over 10-k cv with ridge regression (linear): 21.58114506497026\n",
      "Average testing error over 10-k cv with ridge regression (linear): 34.07824620925931\n",
      "Coefficients\n",
      "    feature  coefficient\n",
      "0      CRIM    -0.150575\n",
      "1        ZN     0.058798\n",
      "2     INDUS    -0.035243\n",
      "3      CHAS     2.424039\n",
      "4       NOX    -8.099813\n",
      "5        RM     3.543438\n",
      "6       AGE    -0.002831\n",
      "7       DIS    -1.472918\n",
      "8       RAD     0.351650\n",
      "9       TAX    -0.015446\n",
      "10  PTRATIO    -0.821765\n",
      "11        B     0.007758\n",
      "12    LSTAT    -0.570019\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "X_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
    "\n",
    "ridge_reg = Ridge()\n",
    "cv_scores_rl = cross_validate(ridge_reg, X, y, cv=10, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "\n",
    "# get train and test MSE from scores dict, multiply by -1 to get positive MSE\n",
    "train_MSE_rl = cv_scores_rl[\"train_score\"] * -1 \n",
    "test_MSE_rl = cv_scores_rl[\"test_score\"] * -1 \n",
    "\n",
    "print(\"Ridge Regression - Linear Features:\")\n",
    "print(\"Average training error over 10-k cv with ridge regression (linear):\", train_MSE_rl.mean())\n",
    "print(\"Average testing error over 10-k cv with ridge regression (linear):\", test_MSE_rl.mean())\n",
    "\n",
    "# Create dataframe with feature names and coefficients\n",
    "ridge_reg.fit(X_train, y_train)\n",
    "ridge_coefs = pd.DataFrame({'feature': X_names, 'coefficient': ridge_reg.coef_})\n",
    "print(\"Coefficients\")\n",
    "print(ridge_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lasso Regression - Linear Features:\n",
      "Average training error over 10-k cv with lasso regression (linear): 26.446016110077032\n",
      "Average testing error over 10-k cv with lasso regression (linear): 34.46408458830231\n",
      "Coefficients\n",
      "    feature  coefficient\n",
      "0      CRIM    -0.100439\n",
      "1        ZN     0.061775\n",
      "2     INDUS    -0.000000\n",
      "3      CHAS     0.000000\n",
      "4       NOX    -0.000000\n",
      "5        RM     0.737827\n",
      "6       AGE     0.025951\n",
      "7       DIS    -0.752248\n",
      "8       RAD     0.334002\n",
      "9       TAX    -0.018118\n",
      "10  PTRATIO    -0.694069\n",
      "11        B     0.006781\n",
      "12    LSTAT    -0.793181\n"
     ]
    }
   ],
   "source": [
    "# Lasso regression\n",
    "lasso_reg = Lasso(max_iter=5000)\n",
    "\n",
    "cv_scores_ll = cross_validate(lasso_reg, X, y, cv=10, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "\n",
    "# get train and test MSE from scores dict, multiply by -1 to get positive MSE\n",
    "train_MSE_ll = cv_scores_ll[\"train_score\"] * -1 \n",
    "test_MSE_ll = cv_scores_ll[\"test_score\"] * -1 \n",
    "\n",
    "print(\"\\nLasso Regression - Linear Features:\")\n",
    "print(\"Average training error over 10-k cv with lasso regression (linear):\", train_MSE_ll.mean())\n",
    "print(\"Average testing error over 10-k cv with lasso regression (linear):\", test_MSE_ll.mean())\n",
    "\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "lasso_coefs = pd.DataFrame({'feature': X_names, 'coefficient': lasso_reg.coef_})\n",
    "print(\"Coefficients\")\n",
    "print(lasso_coefs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression - Polynomial Features:\n",
      "Average training error over 10-k cv with ridge regression (poly): 14.201739336116793\n",
      "Average testing error over 10-k cv with ridge regression (poly): 26.942921093602394\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regression\n",
    "cv_scores_rp = cross_validate(ridge_reg, new_X, y, cv=10, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "\n",
    "# get train and test MSE from scores dict, multiply by -1 to get positive MSE\n",
    "train_MSE_rp = cv_scores_rp[\"train_score\"] * -1 \n",
    "test_MSE_rp = cv_scores_rp[\"test_score\"] * -1 \n",
    "\n",
    "print(\"Ridge Regression - Polynomial Features:\")\n",
    "print(\"Average training error over 10-k cv with ridge regression (poly):\", train_MSE_rp.mean())\n",
    "print(\"Average testing error over 10-k cv with ridge regression (poly):\", test_MSE_rp.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lasso Regression - Polynomial Features:\n",
      "Average training error over 10-k cv with lasso regression (poly): 18.548501008872663\n",
      "Average testing error over 10-k cv with lasso regression (poly): 30.776932337350466\n"
     ]
    }
   ],
   "source": [
    "# Lasso regression\n",
    "cv_scores_lp = cross_validate(lasso_reg, new_X, y, cv=10, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "\n",
    "# get train and test MSE from scores dict, multiply by -1 to get positive MSE\n",
    "train_MSE_lp = cv_scores_lp[\"train_score\"] * -1 \n",
    "test_MSE_lp = cv_scores_lp[\"test_score\"] * -1 \n",
    "\n",
    "print(\"\\nLasso Regression - Polynomial Features:\")\n",
    "print(\"Average training error over 10-k cv with lasso regression (poly):\", train_MSE_lp.mean())\n",
    "print(\"Average testing error over 10-k cv with lasso regression (poly):\", test_MSE_lp.mean())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-size:20px\">Explain which method works better in this case. Check the coefficients and explain the differences between ridge regression and lasso regression..</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (25 points) - Cancer Detection\n",
    "\n",
    "Given a dataset with features that are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass, which describes characteristics of the cell nuclei present in the image, let's try to predict whether the patients are diagnosed as Malignant (M) or Benign (B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA \n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\"\"\"\n",
    "DOCS:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer\n",
    "\"\"\"\n",
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.1 (5 points) \n",
    "Use logistic regression to train the dataset through cross-validation, report the score on train and test set, respectively. Explain your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Recall on Training set: 0.9754116599910991\n",
      "Mean Recall on Testing set: 0.9721428571428572\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model\n",
    "logreg = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# perform 10-fold cross-validation on dataset with recall\n",
    "scores_log = cross_validate(logreg, X, y, scoring='recall', cv=10, return_train_score = True)\n",
    "\n",
    "# get train and test recall from scores dict\n",
    "train_MSE_log = scores_log[\"train_score\"]\n",
    "test_MSE_log = scores_log[\"test_score\"]\n",
    "\n",
    "print(\"Mean Recall on Training set:\", train_MSE_log.mean())\n",
    "print(\"Mean Recall on Testing set:\", test_MSE_log.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-size:20px\">Explain your findings</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.2 (5 points) \n",
    "By default, sklearn's logistic regression uses the L2 regularization. Now use the logistic regression without any regularzation to perform cross validation, report what do you find on train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Recall on Training set: 0.964209283876086\n",
      "Mean Recall on Testing set: 0.9638095238095238\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model without any regularization\n",
    "logreg2 = LogisticRegression(max_iter=10000, penalty='none', solver=\"saga\")\n",
    "\n",
    "# perform 10-fold cross-validation on dataset with recall\n",
    "scores_log2 = cross_validate(logreg2, X, y, cv=10, scoring='recall', return_train_score=True)\n",
    "\n",
    "# print train and test recall from scores dict\n",
    "print(\"Mean Recall on Training set:\", scores_log2[\"train_score\"].mean())\n",
    "print(\"Mean Recall on Testing set:\", scores_log2[\"test_score\"].mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-size:20px\">Now use the logistic regression without any regularzation to perform cross validation, report what do you find on train and test set.</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2.3 (15 points) \n",
    "Check how many Benign and Malignant cases in the dataset. What might be the problem if we use the default score of the logistic regression cross-validation? Now adjust the class weight of M and L and retrain the model again to bias toward Malignant, using the relative weight of M and L as 2:1. What about the relaive weight to be 5:1, or 10:1? Explain what you find.\n",
    "\n",
    "Hint: you can use LogisticRegressionCV to combine LogisticRegression and cross-validation. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding out how many benign and malignant cases are in the dataset\n",
    "\n",
    "see class distribution as stated here: https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 212, 1: 357}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 = negative class = Malignant (=Cancer) = 212\n",
    "\n",
    "1 = positive class = Benign = 357"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of malignant cases: 212 (37.26%)\n",
      "Number of benign cases: 357 (62.74%)\n",
      "Score with 2:1 class weight ratio: 0.96\n",
      "Score with 5:1 class weight ratio: 0.97\n",
      "Score with 10:1 class weight ratio: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Count number of samples in each class\n",
    "n_samples = y.shape[0]\n",
    "n_malignant = sum(y == 0) # 0 = negative class = Malignant (=Cancer)\n",
    "n_benign = sum(y == 1) # 1 = positive class = Benign\n",
    "\n",
    "print(f\"Number of malignant cases: {n_malignant} ({n_malignant / n_samples:.2%})\")\n",
    "print(f\"Number of benign cases: {n_benign} ({n_benign / n_samples:.2%})\")\n",
    "\n",
    "# Define logistic regression model with class weights\n",
    "clf_2to1 = LogisticRegressionCV(class_weight={0: 1, 1: 2}, cv=10, max_iter=10000)\n",
    "clf_5to1 = LogisticRegressionCV(class_weight={0: 1, 1: 5}, cv=10, max_iter=10000)\n",
    "clf_10to1 = LogisticRegressionCV(class_weight={0: 1, 1: 10}, cv=10, max_iter=10000)\n",
    "\n",
    "# train model with 2:1 class weight ratio and evaluate model on test set\n",
    "clf_2to1.fit(X_train, y_train)\n",
    "score_2to1 = clf_2to1.score(X_test, y_test)\n",
    "print(f\"Score with 2:1 class weight ratio: {score_2to1:.2f}\")\n",
    "\n",
    "# Train model with 5:1 class weight ratio and evaluate model on test set\n",
    "clf_5to1.fit(X_train, y_train)\n",
    "score_5to1 = clf_5to1.score(X_test, y_test)\n",
    "print(f\"Score with 5:1 class weight ratio: {score_5to1:.2f}\")\n",
    "\n",
    "# Train model with 10:1 class weight ratio and evaluate model on test set\n",
    "clf_10to1.fit(X_train, y_train)\n",
    "score_10to1 = clf_10to1.score(X_test, y_test)\n",
    "print(f\"Score with 10:1 class weight ratio: {score_10to1:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-size:20px\">What might be the problem if we use the default score of the logistic regression cross-validation? Now adjust the class weight of M and L and retrain the model again to bias toward Malignant, using the relative weight of M and L as 2:1. What about the relaive weight to be 5:1, or 10:1? Explain what you find..</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (50 points) - Call Me Maybe? \n",
    "\n",
    "\n",
    "\n",
    "![telemarketing](https://neilpatel.com/wp-content/uploads/2019/08/profissional-de-telemarketing-sorridente.jpeg)\n",
    "\n",
    "Telemarketing is a method of direct marketing in which a salesperson solicits prospective customers to buy products or services over the phone. It has become one of the most widely used marketing campaign methods to engage with customers with product and service opportunity. We have collected real data from a Portuguese retail bank, from May 2008 to June 2013 with thousands of phone contacts. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The current practice of many data teams is to build such propensity models and predict customer's probability to adopt the product and target them from the highest probability to the lowest probability. Note that telemarketing may incur some costs for contacting the customer, thus the success (i.e., the generated profit) of using machine learning model requries further inspection.  As the data scientist, you are asked to build a propensity model to evaluate the effectiveness of their telemarketing campaigns, i.e. whether the customer subscribed to the term deposit.  \n",
    "\n",
    "**Telemarketing Dataset (bank.csv)**\n",
    "All customers are contained in the file bank.csv. Each line of this file after the header row represents one customer of the Portuguese bank, and has the following format:\n",
    "\n",
    "### bank client data:\n",
    "- age (numeric)\n",
    "- job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "- marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "- education (categorical: 'primary', 'secondary', 'tertiary')\n",
    "- balance: amcount of bank account balance\n",
    "- default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "- housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "- loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "\n",
    "### related with the last contact of the current campaign:\n",
    "- contact: contact communication type (categorical: 'cellular','telephone', 'unknown')\n",
    "- day: last contact day of month\n",
    "- month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "- duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. \n",
    "\n",
    "### other attributes:\n",
    "- campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "- pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; -1 means client was not previously contacted)\n",
    "- previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "- poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "- y - has the client subscribed a term deposit? (binary: 'yes','no')\n",
    "\n",
    "\n",
    "Answer the following questions using the provided dataset. You can write down intermediate results towards the final answers. If any model invovles random_state, set it to be 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>married</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>1787</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>19</td>\n",
       "      <td>oct</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>4789</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>11</td>\n",
       "      <td>may</td>\n",
       "      <td>220</td>\n",
       "      <td>1</td>\n",
       "      <td>339</td>\n",
       "      <td>4</td>\n",
       "      <td>failure</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>management</td>\n",
       "      <td>single</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>1350</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>16</td>\n",
       "      <td>apr</td>\n",
       "      <td>185</td>\n",
       "      <td>1</td>\n",
       "      <td>330</td>\n",
       "      <td>1</td>\n",
       "      <td>failure</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>1476</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>3</td>\n",
       "      <td>jun</td>\n",
       "      <td>199</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>226</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          job  marital  education default  balance housing loan  \\\n",
       "0   30   unemployed  married    primary      no     1787      no   no   \n",
       "1   33     services  married  secondary      no     4789     yes  yes   \n",
       "2   35   management   single   tertiary      no     1350     yes   no   \n",
       "3   30   management  married   tertiary      no     1476     yes  yes   \n",
       "4   59  blue-collar  married  secondary      no        0     yes   no   \n",
       "\n",
       "    contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
       "0  cellular   19   oct        79         1     -1         0  unknown  no  \n",
       "1  cellular   11   may       220         1    339         4  failure  no  \n",
       "2  cellular   16   apr       185         1    330         1  failure  no  \n",
       "3   unknown    3   jun       199         4     -1         0  unknown  no  \n",
       "4   unknown    5   may       226         1     -1         0  unknown  no  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank = pd.read_csv('bank.csv', sep=';')\n",
    "bank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age           int64\n",
       "job          object\n",
       "marital      object\n",
       "education    object\n",
       "default      object\n",
       "balance       int64\n",
       "housing      object\n",
       "loan         object\n",
       "contact      object\n",
       "day           int64\n",
       "month        object\n",
       "duration      int64\n",
       "campaign      int64\n",
       "pdays         int64\n",
       "previous      int64\n",
       "poutcome     object\n",
       "y            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>balance</th>\n",
       "      <th>day</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "      <td>4521.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>41.170095</td>\n",
       "      <td>1422.657819</td>\n",
       "      <td>15.915284</td>\n",
       "      <td>263.961292</td>\n",
       "      <td>2.793630</td>\n",
       "      <td>39.766645</td>\n",
       "      <td>0.542579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.576211</td>\n",
       "      <td>3009.638142</td>\n",
       "      <td>8.247667</td>\n",
       "      <td>259.856633</td>\n",
       "      <td>3.109807</td>\n",
       "      <td>100.121124</td>\n",
       "      <td>1.693562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>-3313.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>444.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>49.000000</td>\n",
       "      <td>1480.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>329.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>87.000000</td>\n",
       "      <td>71188.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>3025.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>871.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age       balance          day     duration     campaign  \\\n",
       "count  4521.000000   4521.000000  4521.000000  4521.000000  4521.000000   \n",
       "mean     41.170095   1422.657819    15.915284   263.961292     2.793630   \n",
       "std      10.576211   3009.638142     8.247667   259.856633     3.109807   \n",
       "min      19.000000  -3313.000000     1.000000     4.000000     1.000000   \n",
       "25%      33.000000     69.000000     9.000000   104.000000     1.000000   \n",
       "50%      39.000000    444.000000    16.000000   185.000000     2.000000   \n",
       "75%      49.000000   1480.000000    21.000000   329.000000     3.000000   \n",
       "max      87.000000  71188.000000    31.000000  3025.000000    50.000000   \n",
       "\n",
       "             pdays     previous  \n",
       "count  4521.000000  4521.000000  \n",
       "mean     39.766645     0.542579  \n",
       "std     100.121124     1.693562  \n",
       "min      -1.000000     0.000000  \n",
       "25%      -1.000000     0.000000  \n",
       "50%      -1.000000     0.000000  \n",
       "75%      -1.000000     0.000000  \n",
       "max     871.000000    25.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no     4000\n",
       "yes     521\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank.y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1 (15 points)\n",
    "\n",
    "Split the data into 80% training set and 20% test set. **Build a pipeline to preprocess the indicated numerical features and categorical features separately**. For numerical features 'balance', 'campaign', standardize these features. For categorical features 'job', 'marital', 'education', 'default', transform them through one-hot encoding. For the numeric feature 'age', convert it into the quartile categorical variable and transform it through one-hot encoding. \n",
    "\n",
    "Train a Logistic regression model with L2 regularization using 5-fold cross validation (default hyperparameter) on the train set and show the accuracy, precision, recall on the train set. Explain whether the model is useful for the bank to identify the customer propensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bank.drop('y', axis = 1)\n",
    "y = bank[\"y\"] .replace({\"yes\": 1, \"no\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8832962716734294\n",
      "Precision: 0.6\n",
      "Recall: 0.009439775910364146\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "\n",
    "# define attributes\n",
    "num_attribs = [\"balance\", \"campaign\"]\n",
    "cat_attribs = [\"job\", \"marital\", \"education\", \"default\"]\n",
    "\n",
    "# define pipeline to apply feature scaling to numerical features\n",
    "num_pipeline = Pipeline([\n",
    "        ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# define pipeline to apply one-hot encoding to categorical values\n",
    "cat_pipeline = Pipeline([\n",
    "        ('cat_encoder', OneHotEncoder(sparse=False, handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "# define pipeline to convert age into the quartile categorical variable and transform it through one-hot encoding. \n",
    "age_pipeline = Pipeline([\n",
    "        ('quartile_transformer', QuantileTransformer(n_quantiles=4)),\n",
    "        ('cat_encoder', OneHotEncoder(sparse=False, handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "# apply different operations on num and cat by combining pipelines with ColumnTransformer\n",
    "preprocessing = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", cat_pipeline, cat_attribs),\n",
    "        (\"age\", age_pipeline, [\"age\"])\n",
    "    ])\n",
    "\n",
    "# append logistic regression to preprocessing pipeline\n",
    "full_pipeline_l2 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessing),\n",
    "    ('classifier', LogisticRegression(penalty='l2'))\n",
    "])\n",
    "\n",
    "# split data into training and testing data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=seed)\n",
    "\n",
    "# perform cross-validation on the training data\n",
    "scoring = ['accuracy', 'precision', 'recall']\n",
    "cv_scores = cross_validate(full_pipeline_l2, X_train, y_train, cv=5, scoring=scoring)\n",
    "\n",
    "# display mean accuracy, precision, and recall over the 5 folds\n",
    "print(\"Accuracy:\", cv_scores['test_accuracy'].mean())\n",
    "print(\"Precision:\", cv_scores['test_precision'].mean())\n",
    "print(\"Recall:\", cv_scores['test_recall'].mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPLAIN...!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2 (10 points)\n",
    "\n",
    "Now add more features to the model to see if we can improve the performance (categorical features: 'housing', 'loan' and numerical features: 'day', 'duration'). Use the preprocess pipeline built previously to transform the data. \n",
    "\n",
    "Train a Logistic regression model with L1 regularization using 5-fold cross validation on the train set, by fine-tuning the hyperparameter alpha, i.e. the regularization strength from [0.001, 0.01, 0.1, 1]. Choose the correct score function that reflect the current data team's practice. Report the average score with the best hyperparameter. Does model performance improve, and if so, how?\n",
    "\n",
    "Expalin whether all features are useful for making prediction and why. List top 5 features that contribute to the prediction the most. If not all features are useful, list those unuseful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add more attributes\n",
    "num_attribs = [\"balance\", \"campaign\", \"day\", \"duration\"]\n",
    "cat_attribs = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\"]\n",
    "\n",
    "# append logistic regression to preprocessing pipeline\n",
    "full_pipeline_l1 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessing),\n",
    "    ('classifier', LogisticRegression(penalty='l1'))\n",
    "])\n",
    "\n",
    "# split data into training and testing data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=seed)\n",
    "\n",
    "# Use the GridSearchCV object with cross_val_score to perform cross-validation and hyperparameter tuning\n",
    "params = {'C': [0.001, 0.01, 0.1, 1]}\n",
    "grid_search = GridSearchCV(full_pipeline_l1, params)\n",
    "\n",
    "#grid_search.best_params_\n",
    "# scoring = ['accuracy', 'precision', 'recall']\n",
    "# cv_scores2 = cross_validate(grid_search, X_train, y_train, cv=5, scoring=scoring)\n",
    "\n",
    "# cv_scores2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3 (10 points)\n",
    "\n",
    "Now use the best model found in the cross-validation to predict the test set, show the obtained confusion matrix. Assume that targeting each customer would cost 10 euros and if the customer subscribe, the company would earn 50 euros. If we perform targeted telemarketing to all customers that are predicted to subscribe in the test set, what's the resulting profit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.4 (10 points)\n",
    "\n",
    "Now adjust the decision threshold in order to optimize the obtained profit. What would be the resulting threshold and profit? Is the propensity model built based on the targeting predicted probability useful in terms of profit maximizing? Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.5 (5 points)\n",
    "\n",
    "Now train a random forest model, with 10 decision trees and max_depth=2, what is the profit that can be achieved given the threshold that you identified earlier? Do you need to increase or decrese the threshold to maximize the profit using random forest model? Explain your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a81a1412cade015a653d21208291244715c6a021abd2ef05b33165e8b2673496"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
